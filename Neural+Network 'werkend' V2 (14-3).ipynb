{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 148.   85.  183.   89.  137.  116.   78.  115.  197.  125.  110.  168.\n",
      "  139.  189.  166.  100.  118.  107.  103.  115.  126.   99.  196.  119.\n",
      "  143.  125.  147.   97.  145.  117.  109.  158.   88.   92.  122.  103.\n",
      "  138.  102.   90.  111.  180.  133.  106.  171.  159.  180.  146.   71.\n",
      "  103.  105.  103.  101.   88.  176.  150.   73.  187.  100.  146.  105.\n",
      "   84.  133.   44.  141.  114.   99.  109.  109.   95.  146.  100.  139.\n",
      "  126.  129.   79.    0.   62.   95.  131.  112.  113.   74.   83.  101.\n",
      "  137.  110.  106.  100.  136.  107.   80.  123.   81.  134.  142.  144.\n",
      "   92.   71.   93.  122.  163.  151.  125.   81.   85.  126.   96.  144.\n",
      "   83.   95.  171.  155.   89.   76.  160.  146.  124.   78.   97.   99.\n",
      "  162.  111.  107.  132.  113.   88.  120.  118.  117.  105.  173.  122.\n",
      "  170.   84.   96.  125.  100.   93.  129.  105.  128.  106.  108.  108.\n",
      "  154.  102.   57.  106.  147.   90.  136.  114.  156.  153.  188.  152.\n",
      "   99.  109.   88.  163.  151.  102.  114.  100.  131.  104.  148.  120.\n",
      "  110.  111.  102.  134.   87.   79.   75.  179.   85.  129.  143.  130.\n",
      "   87.  119.    0.   73.  141.  194.  181.  128.  109.  139.  111.  123.\n",
      "  159.  135.   85.  158.  105.  107.  109.  148.  113.  138.  108.   99.\n",
      "  103.  111.  196.  162.   96.  184.   81.  147.  179.  140.  112.  151.\n",
      "  109.  125.   85.  112.  177.  158.  119.  142.  100.   87.  101.  162.\n",
      "  197.  117.  142.  134.   79.  122.   74.  171.  181.  179.  164.  104.\n",
      "   91.   91.  139.  119.  146.  184.  122.  165.  124.  111.  106.  129.\n",
      "   90.   86.   92.  113.  111.  114.  193.  155.  191.  141.   95.  142.\n",
      "  123.   96.  138.  128.  102.  146.  101.  108.  122.   71.  106.  100.\n",
      "  106.  104.  114.  108.  146.  129.  133.  161.  108.  136.  155.  119.\n",
      "   96.  108.   78.  107.  128.  128.  161.  151.  146.  126.  100.  112.\n",
      "  167.  144.   77.  115.  150.  120.  161.  137.  128.  124.   80.  106.\n",
      "  155.  113.  109.  112.   99.  182.  115.  194.  129.  112.  124.  152.\n",
      "  112.  157.  122.  179.  102.  105.  118.   87.  180.  106.   95.  165.\n",
      "  117.  115.  152.  178.  130.   95.    0.  122.   95.  126.  139.  116.\n",
      "   99.    0.   92.  137.   61.   90.   90.  165.  125.  129.   88.  196.\n",
      "  189.  158.  103.  146.  147.   99.  124.  101.   81.  133.  173.  118.\n",
      "   84.  105.  122.  140.   98.   87.  156.   93.  107.  105.  109.   90.\n",
      "  125.  119.  116.  105.  144.  100.  100.  166.  131.  116.  158.  127.\n",
      "   96.  131.   82.  193.   95.  137.  136.   72.  168.  123.  115.  101.\n",
      "  197.  172.  102.  112.  143.  143.  138.  173.   97.  144.   83.  129.\n",
      "  119.   94.  102.  115.  151.  184.   94.  181.  135.   95.   99.   89.\n",
      "   80.  139.   90.  141.  140.  147.   97.  107.  189.   83.  117.  108.\n",
      "  117.  180.  100.   95.  104.  120.   82.  134.   91.  119.  100.  175.\n",
      "  135.   86.  148.  134.  120.   71.   74.   88.  115.  124.   74.   97.\n",
      "  120.  154.  144.  137.  119.  136.  114.  137.  105.  114.  126.  132.\n",
      "  158.  123.   85.   84.  145.  135.  139.  173.   99.  194.   83.   89.\n",
      "   99.  125.   80.  166.  110.   81.  195.  154.  117.   84.    0.   94.\n",
      "   96.   75.  180.  130.   84.  120.   84.  139.   91.   91.   99.  163.\n",
      "  145.  125.   76.  129.   68.  124.  114.  130.  125.   87.   97.  116.\n",
      "  117.  111.  122.  107.   86.   91.   77.  132.  105.   57.  127.  129.\n",
      "  100.  128.   90.   84.   88.  186.  187.  131.  164.  189.  116.   84.\n",
      "  114.   88.   84.  124.   97.  110.  103.   85.  125.  198.   87.   99.\n",
      "   91.   95.   99.   92.  154.  121.   78.  130.  111.   98.  143.  119.\n",
      "  108.  118.  133.  197.  151.  109.  121.  100.  124.   93.  143.  103.\n",
      "  176.   73.  111.  112.  132.   82.  123.  188.   67.   89.  173.  109.\n",
      "  108.   96.  124.  150.  183.  124.  181.   92.  152.  111.  106.  174.\n",
      "  168.  105.]\n",
      "614\n",
      "154\n",
      "768\n",
      "[ 148.   85.  183.   89.  137.  116.   78.  115.  197.  125.  110.  168.\n",
      "  139.  189.  166.  100.  118.  107.  103.  115.  126.   99.  196.  119.\n",
      "  143.  125.  147.   97.  145.  117.  109.  158.   88.   92.  122.  103.\n",
      "  138.  102.   90.  111.  180.  133.  106.  171.  159.  180.  146.   71.\n",
      "  103.  105.  103.  101.   88.  176.  150.   73.  187.  100.  146.  105.\n",
      "   84.  133.   44.  141.  114.   99.  109.  109.   95.  146.  100.  139.\n",
      "  126.  129.   79.    0.   62.   95.  131.  112.  113.   74.   83.  101.\n",
      "  137.  110.  106.  100.  136.  107.   80.  123.   81.  134.  142.  144.\n",
      "   92.   71.   93.  122.  163.  151.  125.   81.   85.  126.   96.  144.\n",
      "   83.   95.  171.  155.   89.   76.  160.  146.  124.   78.   97.   99.\n",
      "  162.  111.  107.  132.  113.   88.  120.  118.  117.  105.  173.  122.\n",
      "  170.   84.   96.  125.  100.   93.  129.  105.  128.  106.  108.  108.\n",
      "  154.  102.   57.  106.  147.   90.  136.  114.  156.  153.  188.  152.\n",
      "   99.  109.   88.  163.  151.  102.  114.  100.  131.  104.  148.  120.\n",
      "  110.  111.  102.  134.   87.   79.   75.  179.   85.  129.  143.  130.\n",
      "   87.  119.    0.   73.  141.  194.  181.  128.  109.  139.  111.  123.\n",
      "  159.  135.   85.  158.  105.  107.  109.  148.  113.  138.  108.   99.\n",
      "  103.  111.  196.  162.   96.  184.   81.  147.  179.  140.  112.  151.\n",
      "  109.  125.   85.  112.  177.  158.  119.  142.  100.   87.  101.  162.\n",
      "  197.  117.  142.  134.   79.  122.   74.  171.  181.  179.  164.  104.\n",
      "   91.   91.  139.  119.  146.  184.  122.  165.  124.  111.  106.  129.\n",
      "   90.   86.   92.  113.  111.  114.  193.  155.  191.  141.   95.  142.\n",
      "  123.   96.  138.  128.  102.  146.  101.  108.  122.   71.  106.  100.\n",
      "  106.  104.  114.  108.  146.  129.  133.  161.  108.  136.  155.  119.\n",
      "   96.  108.   78.  107.  128.  128.  161.  151.  146.  126.  100.  112.\n",
      "  167.  144.   77.  115.  150.  120.  161.  137.  128.  124.   80.  106.\n",
      "  155.  113.  109.  112.   99.  182.  115.  194.  129.  112.  124.  152.\n",
      "  112.  157.  122.  179.  102.  105.  118.   87.  180.  106.   95.  165.\n",
      "  117.  115.  152.  178.  130.   95.    0.  122.   95.  126.  139.  116.\n",
      "   99.    0.   92.  137.   61.   90.   90.  165.  125.  129.   88.  196.\n",
      "  189.  158.  103.  146.  147.   99.  124.  101.   81.  133.  173.  118.\n",
      "   84.  105.  122.  140.   98.   87.  156.   93.  107.  105.  109.   90.\n",
      "  125.  119.  116.  105.  144.  100.  100.  166.  131.  116.  158.  127.\n",
      "   96.  131.   82.  193.   95.  137.  136.   72.  168.  123.  115.  101.\n",
      "  197.  172.  102.  112.  143.  143.  138.  173.   97.  144.   83.  129.\n",
      "  119.   94.  102.  115.  151.  184.   94.  181.  135.   95.   99.   89.\n",
      "   80.  139.   90.  141.  140.  147.   97.  107.  189.   83.  117.  108.\n",
      "  117.  180.  100.   95.  104.  120.   82.  134.   91.  119.  100.  175.\n",
      "  135.   86.  148.  134.  120.   71.   74.   88.  115.  124.   74.   97.\n",
      "  120.  154.  144.  137.  119.  136.  114.  137.  105.  114.  126.  132.\n",
      "  158.  123.   85.   84.  145.  135.  139.  173.   99.  194.   83.   89.\n",
      "   99.  125.   80.  166.  110.   81.  195.  154.  117.   84.    0.   94.\n",
      "   96.   75.  180.  130.   84.  120.   84.  139.   91.   91.   99.  163.\n",
      "  145.  125.   76.  129.   68.  124.  114.  130.  125.   87.   97.  116.\n",
      "  117.  111.  122.  107.   86.   91.   77.  132.  105.   57.  127.  129.\n",
      "  100.  128.   90.   84.   88.  186.  187.  131.  164.  189.  116.   84.\n",
      "  114.   88.   84.  124.   97.  110.  103.   85.  125.  198.   87.   99.\n",
      "   91.   95.   99.   92.  154.  121.   78.  130.  111.   98.  143.  119.\n",
      "  108.  118.  133.  197.  151.  109.  121.  100.  124.   93.  143.  103.\n",
      "  176.   73.  111.  112.  132.   82.  123.  188.   67.   89.  173.  109.\n",
      "  108.   96.  124.  150.  183.  124.  181.   92.  152.  111.  106.  174.\n",
      "  168.  105.]\n",
      "(614,) (154,)\n"
     ]
    }
   ],
   "source": [
    "#here we read the data where X is the input and Y is the output\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "length = len(dataset.values)\n",
    "width = dataset.ndim-1\n",
    "ratio = int(length*0.8) #training ratio (includes validation) vs test ratio \n",
    "\n",
    "x_train = dataset.values[0:ratio,0:width]\n",
    "y_train = dataset.values[0:ratio:,width]\n",
    "\n",
    "x_test = dataset.values[ratio:,0:width]\n",
    "y_test = dataset.values[ratio:,width]\n",
    "\n",
    "print (y_train)\n",
    "\n",
    "#y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)\n",
    "\n",
    "#checking lengths\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(x_train)+len(x_test))\n",
    "\n",
    "print (y_train)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "#scatterplots\n",
    "#plt.scatter(creditcard.Class, creditcard.V2, color='red')\n",
    "#plt.scatter(creditcard.Class, creditcard.V5, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_65 (Dense)             (None, 12)                180       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 293\n",
      "Trainable params: 293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#here we create the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "model = Sequential()\n",
    "#layers\n",
    "model.add(Dense(12, input_dim=14, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "optimizer = Adam(lr=0.1) # lr is the learning rate\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_65_input to have shape (14,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-a2fb76723eac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#here we train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1636\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1637\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1638\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1481\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1483\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1484\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    121\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_65_input to have shape (14,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "#here we train the model\n",
    "model.fit(x_train, y_train, epochs=15, batch_size=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 511 samples, validate on 103 samples\n",
      "Epoch 1/5\n",
      "511/511 [==============================] - 0s 271us/step - loss: -1911.3079 - acc: 0.0000e+00 - val_loss: -1869.4382 - val_acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "511/511 [==============================] - 0s 97us/step - loss: -1911.3079 - acc: 0.0000e+00 - val_loss: -1869.4382 - val_acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "511/511 [==============================] - 0s 88us/step - loss: -1911.3079 - acc: 0.0000e+00 - val_loss: -1869.4382 - val_acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "511/511 [==============================] - 0s 74us/step - loss: -1911.3079 - acc: 0.0000e+00 - val_loss: -1869.4382 - val_acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "511/511 [==============================] - 0s 78us/step - loss: -1911.3079 - acc: 0.0000e+00 - val_loss: -1869.4382 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x49920ebe10>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we validate the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7580 - acc: 0.6420\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 0s 320us/step - loss: 0.6429 - acc: 0.7120\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 0s 285us/step - loss: 0.5695 - acc: 0.7300\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 0s 286us/step - loss: 0.6096 - acc: 0.7260\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 0s 295us/step - loss: 0.5392 - acc: 0.7380\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 0s 346us/step - loss: 0.5331 - acc: 0.7180\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 0s 308us/step - loss: 0.4883 - acc: 0.7620\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 0s 283us/step - loss: 0.5516 - acc: 0.7100\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 0s 306us/step - loss: 0.4698 - acc: 0.7760\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 0s 288us/step - loss: 0.4944 - acc: 0.7600\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 0s 362us/step - loss: 0.4795 - acc: 0.7560\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 0s 402us/step - loss: 0.5104 - acc: 0.7360\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 0s 384us/step - loss: 0.4737 - acc: 0.7720\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 0s 430us/step - loss: 0.4838 - acc: 0.7360\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 0s 286us/step - loss: 0.4759 - acc: 0.7320\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 0s 406us/step - loss: 0.4493 - acc: 0.7660\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 0s 388us/step - loss: 0.4427 - acc: 0.7860\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 0s 334us/step - loss: 0.4510 - acc: 0.7640\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 0s 285us/step - loss: 0.4394 - acc: 0.7700\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 0s 300us/step - loss: 0.4186 - acc: 0.7860\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 0s 316us/step - loss: 0.4049 - acc: 0.8020\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 0s 297us/step - loss: 0.4099 - acc: 0.7980\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 0s 306us/step - loss: 0.4309 - acc: 0.7760\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 0s 301us/step - loss: 0.4032 - acc: 0.7840\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 0s 292us/step - loss: 0.3956 - acc: 0.8060\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 0s 310us/step - loss: 0.4599 - acc: 0.7520\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 0s 293us/step - loss: 0.3982 - acc: 0.7960\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 0s 290us/step - loss: 0.3912 - acc: 0.8140\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 0s 304us/step - loss: 0.3982 - acc: 0.8100\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 0s 307us/step - loss: 0.3932 - acc: 0.7860\n",
      "Train on 416 samples, validate on 84 samples\n",
      "Epoch 1/5\n",
      "416/416 [==============================] - 1s 2ms/step - loss: 0.3675 - acc: 0.8197 - val_loss: 0.2681 - val_acc: 0.8929\n",
      "Epoch 2/5\n",
      "416/416 [==============================] - 0s 165us/step - loss: 0.3538 - acc: 0.8365 - val_loss: 0.2789 - val_acc: 0.9048\n",
      "Epoch 3/5\n",
      "416/416 [==============================] - 0s 125us/step - loss: 0.3329 - acc: 0.8389 - val_loss: 0.2785 - val_acc: 0.9048\n",
      "Epoch 4/5\n",
      "416/416 [==============================] - 0s 123us/step - loss: 0.3259 - acc: 0.8413 - val_loss: 0.2931 - val_acc: 0.8810\n",
      "Epoch 5/5\n",
      "416/416 [==============================] - 0s 127us/step - loss: 0.3184 - acc: 0.8462 - val_loss: 0.3076 - val_acc: 0.8810\n",
      "0 178 6 84\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "dataset = pd.read_csv('./diabetes.csv', delimiter=\",\")\n",
    "\n",
    "split = 500\n",
    "\n",
    "Xtrain = dataset.values[:split,0:8]\n",
    "Ytrain = dataset.values[:split,8]\n",
    "Xtest = dataset.values[split:,0:8]\n",
    "Ytest = dataset.values[split:,8]\n",
    "\n",
    "pca = PCA(n_components = 8)\n",
    "pca.fit(Xtrain)\n",
    "\n",
    "Xtrain = pca.transform(Xtrain)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=8, activation='relu'))\n",
    "model.add(Dense(235, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(16, activation='selu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = Adam(lr=0.001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.fit(Xtrain, Ytrain, epochs=30, batch_size=10)\n",
    "model.fit(Xtrain, Ytrain, epochs=5, batch_size=32, validation_split=1/6)\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "# calculate predictions\n",
    "predictions = [round(x[0]) for x in model.predict(Xtest)]\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == Ytest[i]:\n",
    "        if predictions[1] == 1:\n",
    "            TP += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "    else:\n",
    "        if predictions[i] == 1:\n",
    "            FP += 1\n",
    "        else:\n",
    "            FN += 1\n",
    "            \n",
    "\n",
    "print(TP, TN, FP, FN)            \n",
    "            \n",
    "#print ('Precision is =', TP/(TP+FP))\n",
    "#print ('Recall is =', TP/(TP + TN))\n",
    "#print ('Accuracy is =', TP+TN/(len(predictions)))\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
